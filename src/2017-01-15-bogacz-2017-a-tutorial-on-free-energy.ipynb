{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I enjoyed reading \"A tutorial on the free-energy framework for modelling perception and learning\" by *Rafal Bogacz*, which is freely available [here](http://www.sciencedirect.com/science/article/pii/S0022249615000759). In particular, the author encourages to replicate the results in the paper. He is himself giving solutions in matlab, so I had to do the same in python all within a notebook... \n",
    "\n",
    "<!-- TEASER_END -->\n",
    "\n",
    "Let's first initialize the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:16.164578Z",
     "start_time": "2018-03-27T09:13:15.895717Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=6, suppress=True)\n",
    "%matplotlib notebook\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import matplotlib.pyplot as plt\n",
    "phi = (np.sqrt(5)+1)/2\n",
    "fig_width = 10\n",
    "figsize = (fig_width, fig_width/phi)\n",
    "do_save = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 1 : defining probabilities\n",
    "\n",
    "\n",
    "First, let's see the application of Bayes theorem \n",
    "We start by considering in this section a simple perceptual problem in which a value of a single variable has to be inferred from a single observation. To make it more concrete, consider a simple organism that tries to infer the size or diameter of a food item, which we denote by $v$, on the basis of light intensity it observes. Let us assume that our simple animal has only one light sensitive receptor which provides it with a noisy estimate of light intensity, which we denote by $u$. Let g\n",
    " denote a non-linear function relating the average light intensity with the size. Since the amount of light reflected is related to the area of an object, in this example we will consider a simple function of $g(v)=v^2$. Let us further assume that the sensory input is noisy—in particular, when the size of food item is v, the perceived light intensity is normally distributed with mean g(v)\n",
    ", and variance $Σ_u$ (although a normal distribution is not the best choice for a distribution of light intensity, as it includes negative numbers, we will still use it for a simplicity):\n",
    "$$\n",
    "p(u|v)=f(u; g(v), Σ_u),\n",
    "$$\n",
    "where $f(x;μ,Σ) $ denotes by definition the density of a normal distribution with mean μ  and variance Σ \n",
    "\n",
    "Due to the noise present in the observed light intensity, the animal can refine its guess for the size v\n",
    " by combining the sensory stimulus with the prior knowledge on how large the food items usually are, that it had learnt from experience. For simplicity, let us assume that our animal expects this size to be normally distributed with mean $v_p$  and variance $Σ_p$  (subscript p stands for “prior”), which we can write as:\n",
    "$$\n",
    "p(v)=f(v; vp, Σ_p).\n",
    "$$\n",
    "\n",
    "## Exact solution\n",
    "\n",
    "To compute how likely different sizes $v$ are given the observed sensory input $u$, we could use Bayes’ theorem:\n",
    "$$\n",
    "p(v|u)=p(v)p(u|v)p(u). \n",
    "$$\n",
    "\n",
    "Term $p(u)$  in the denominator of equation is a normalization term, which ensures that the posterior probabilities of all sizes $p(v|u)$  integrate to 1:\n",
    "\n",
    "$$\n",
    "p(u)= \\int p(v) p(u|v) dv.\n",
    "$$\n",
    "\n",
    "The integral in the above equation sums over the whole range of possible values of $v$, so it is a definite integral, but for brevity of notation we do not state the limits of integration in this and all other integrals in the paper.\n",
    "\n",
    "Now combining Eqs. we can compute numerically how likely different sizes are given the sensory observation. For readers who are not familiar with such Bayesian inference we recommend doing the following exercise now.\n",
    "\n",
    "### solution to Exercise 1.\n",
    "\n",
    "Assume that our animal observed the light intensity  $u=2$, the level of noise in its receptor is  $Σ_u=1$, and the mean and variance of its prior expectation of size are  $v_p=3$ and  $Σ_p=1$. Write a computer program that computes the posterior probabilities of sizes from  0.01  to  5, and plots them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:16.168986Z",
     "start_time": "2018-03-27T09:13:16.166323Z"
    }
   },
   "outputs": [],
   "source": [
    "u_obs = 2 # observation\n",
    "var_u = 1 # noise in the observation\n",
    "v_p = 3 # prior expectation\n",
    "var_p = 1 # variance of prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:19.631621Z",
     "start_time": "2018-03-27T09:13:16.170852Z"
    }
   },
   "outputs": [],
   "source": [
    "def gauss(x, mean, variance):\n",
    "    return 1 / np.sqrt(2* np.pi * variance) * np.exp(- .5 * (x - mean)**2 / variance )\n",
    "\n",
    "g = lambda v: v**2\n",
    "\n",
    "sizes = np.linspace(0.01, 5, 100)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=figsize)\n",
    "\n",
    "prior = gauss(sizes, v_p, var_p)\n",
    "axs[0].plot(sizes, prior, 'k')\n",
    "axs[0].set_title('Prior')\n",
    "\n",
    "\n",
    "for var_u_ in np.logspace(-1, 1, 7, base=10)*var_u:\n",
    "    likelihood = gauss(u_obs, g(sizes), var_u_)\n",
    "    axs[1].plot(sizes, likelihood/likelihood.sum())\n",
    "    axs[1].set_title('Likelihood')\n",
    "\n",
    "\n",
    "    posterior = prior * likelihood\n",
    "    posterior /= posterior.sum()\n",
    "    axs[2].plot(sizes, posterior, label=r'$\\sigma_u^2$ ={0:.2f}'.format(var_u_))\n",
    "    axs[2].set_title('Posterior')\n",
    "\n",
    "axs[2].legend()\n",
    "for ax in axs:\n",
    "    ax.set_xlabel('Size')\n",
    "    ax.set_ylabel('Probability')\n",
    "\n",
    "plt.tight_layout()\n",
    "if do_save == False: fig.savefig('../figures/bogacz_1.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# exercise 2 : an online solution\n",
    "\n",
    "Let's define $F = \\log( p(u |v) )$\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial F}{\\partial v} = \\frac{v - v_p}{\\Sigma_p} + \\dot{g}(v) \\cdot \\frac{u - g(v)}{\\Sigma_u}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:19.663182Z",
     "start_time": "2018-03-27T09:13:19.633529Z"
    }
   },
   "outputs": [],
   "source": [
    "dg = lambda v: 2*v\n",
    "\n",
    "T, dt = 5, 0.01\n",
    "times = np.linspace(0., T, int(T/dt))\n",
    "\n",
    "v = np.zeros_like(times)\n",
    "for i_time, time in enumerate(times):\n",
    "    if time == 0 :\n",
    "        v[i_time] = v_p\n",
    "    else:\n",
    "        v[i_time] = v[i_time-1] + dt * ( (v[i_time-1] -v_p) / var_p + dg(v[i_time-1]) * (u_obs-g(v[i_time-1] )) / var_u )\n",
    "\n",
    "\n",
    "# now going online\n",
    "v = np.zeros_like(times)\n",
    "eps_u = np.zeros_like(times)\n",
    "eps_p = np.zeros_like(times)\n",
    "\n",
    "for i_time, time in enumerate(times):\n",
    "    if time == 0 :\n",
    "        v[i_time], eps_u[i_time], eps_p[i_time] = v_p, 0., 0.\n",
    "    else:\n",
    "        v[i_time] = v[i_time-1] + dt * ( - eps_p[i_time-1] + dg(v[i_time-1]) * eps_u[i_time-1] )\n",
    "\n",
    "        eps_p[i_time] = eps_p[i_time-1] + dt * ( (v[i_time] -v_p) - var_p * eps_p[i_time-1] )\n",
    "        eps_u[i_time] = eps_u[i_time-1] + dt * ( (u_obs-g(v[i_time])) - var_u * eps_u[i_time-1]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:22.638208Z",
     "start_time": "2018-03-27T09:13:19.664998Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, [ax1, ax2] = plt.subplots(1, 2, figsize=figsize)\n",
    "ax1.plot(times, v)\n",
    "ax1.set_xlabel('Time (s)')\n",
    "ax1.set_ylabel('Size')\n",
    "ax1.set_ylim(0, 5)\n",
    "ax2.plot(times, v, label=r'$\\phi$')\n",
    "ax2.set_xlabel('Time (s)')\n",
    "ax2.set_ylabel('Size')\n",
    "ax2.plot(times, eps_p, 'g--', label=r'$\\epsilon_p$')\n",
    "ax2.plot(times, eps_u, 'r--', label=r'$\\epsilon_u$')\n",
    "ax2.legend();\n",
    "if do_save == False: fig.savefig('../figures/bogacz_2.png', dpi=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# exercise 5 : estimating variance\n",
    "\n",
    "\n",
    "\n",
    "one may also learn the variance\n",
    "\n",
    "The model parameters can be hence optimized by modifying them proportionally to the gradient of  $F$. It is straightforward to find the derivatives of  F  over  $v_p$, $Σ_p$  and  $Σ_u$:\n",
    "\n",
    "$$\n",
    "\\frac{∂F}{∂v_p}=\\frac{ϕ−v_p}{Σ_p}\n",
    "$$\n",
    "$$\n",
    "\\frac{∂F}{∂Σp}=\\frac 1 2 ( \\frac {(ϕ−v_p)^2} {Σ^2_p}−\\frac{1}{Σ_p} )\n",
    "$$\n",
    "$$\n",
    "\\frac{∂F}{∂Σu}=\\frac 1 2 ( \\frac {(u−g(ϕ))^2}{Σ^2_u}−\\frac{1}{Σ_u}. \n",
    "$$\n",
    "\n",
    "From the paper:\n",
    "\n",
    "\n",
    "> Simulate learning of variance  $Σ_i$ over trials. For simplicity, only simulate the network described by Eqs. (59)– (60), and assume that variables  ϕ are constant. On each trial generate input  $ϕ_i$ from a normal distribution with mean  5  and variance  2, while set  $g_i(ϕ_i+1)=5$ (so that the upper level correctly predicts the mean of  $ϕ_i$). Simulate the network for  20  time units, and then update weight  $Σ_i$ with learning rate  $α=0.01$. Simulate  1000  trials and plot how $Σ_i$ changes across trials.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:32.289964Z",
     "start_time": "2018-03-27T09:13:22.639920Z"
    }
   },
   "outputs": [],
   "source": [
    "mean_u_obs = 5 # observation\n",
    "var_u = 2 # noise in the observation\n",
    "var_u_init = 1. # initial guess\n",
    "v_p = 5 # prior expectation (from node above)\n",
    "var_p = 1 # variance of prior (from node above)\n",
    "\n",
    "eta = .01\n",
    "N_trials = 1000\n",
    "\n",
    "T, dt = 50, 0.01\n",
    "times = np.linspace(0., T, int(T/dt))\n",
    "\n",
    "\n",
    "v = np.zeros_like(times)\n",
    "e = np.zeros_like(times)\n",
    "error = np.zeros_like(times)\n",
    "\n",
    "var_u_ = var_u_init * np.ones(N_trials)\n",
    "\n",
    "for i_trial in range(1, N_trials):\n",
    "    # making an observation\n",
    "    u_obs = mean_u_obs + np.sqrt(var_u) * np.random.randn()\n",
    "\n",
    "    for i_time, time in enumerate(times):\n",
    "        if time == 0 :\n",
    "            e[i_time], error[i_time] = 0., 0.\n",
    "        else:\n",
    "            error[i_time] = error[i_time-1] + dt * ( (u_obs - v_p) - var_p * e[i_time-1] )\n",
    "            e[i_time] = e[i_time-1] + dt * (var_u_[i_trial-1] * error[i_time-1] - e[i_time-1])\n",
    "\n",
    "    var_u_[i_trial] = var_u_[i_trial-1] + eta * (error[-1]*e[-1] - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-03-27T09:13:35.296731Z",
     "start_time": "2018-03-27T09:13:32.291524Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=figsize)\n",
    "ax.plot(var_u_, label='estimate')\n",
    "ax.plot(var_u_init*np.ones_like(var_u_), 'g--', label='initial guess')\n",
    "ax.plot(var_u*np.ones_like(var_u_), 'r--', label='(hidden) true value')\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.set_xlabel('trials')\n",
    "ax.set_ylabel(r'$\\Sigma$');\n",
    "ax.legend()\n",
    "if do_save == False: fig.savefig('../figures/bogacz_3.png', dpi=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
